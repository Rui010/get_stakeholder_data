from datetime import datetime, timedelta
import os
from dotenv import load_dotenv
import requests
import zipfile
import io
import xml.etree.ElementTree as ET
from itertools import zip_longest
from bs4 import BeautifulSoup
import re


def extract_major_shareholders_from_html(root, ns):
    block = root.find(".//jp:MajorShareholdersTextBlock", ns)
    if block is None or not block.text:
        return []

    soup = BeautifulSoup(block.text, "html.parser")
    rows = soup.find_all("tr")

    result = []
    for row in rows:
        cols = [td.get_text(strip=True) for td in row.find_all("td")]
        if len(cols) >= 3:
            result.append({"æ°å": cols[0], "ä¿æœ‰æ ªæ•°": cols[1], "ä¿æœ‰å‰²åˆ": cols[2]})
    return result


def parse_xbrl_latest(xbrl_bytes):
    ns = {
        "jp": "http://disclosure.edinet-fsa.go.jp/taxonomy/jpcrp/2023-12-01/jpcrp_cor"
    }
    root = ET.fromstring(xbrl_bytes)

    def clean_text(text):
        import re

        if not text:
            return ""
        text = text.replace("\n", "").replace("\r", "")
        text = re.sub(r"\s+", " ", text)
        text = text.replace("\u3000", " ")
        return text.strip()

    # ä»£è¡¨è€…
    rep = root.find(".//jp:TitleAndNameOfRepresentativeCoverPage", ns)
    representative = clean_text(rep.text) if rep is not None and rep.text else None

    major_shareholders = extract_major_shareholders_from_html(root, ns)

    # å½¹å“¡
    names = root.findall(".//jp:NameInformationAboutDirectorsAndCorporateAuditors", ns)
    titles = root.findall(
        ".//jp:OfficialTitleOrPositionInformationAboutDirectorsAndCorporateAuditors", ns
    )
    births = root.findall(
        ".//jp:DateOfBirthInformationAboutDirectorsAndCorporateAuditors", ns
    )

    officers = []
    for name, title, birth in zip(names, titles, births):
        officers.append(
            {
                "æ°å": clean_text(name.text),
                "è‚©æ›¸ã": clean_text(title.text),
                "ç”Ÿå¹´æœˆæ—¥": clean_text(birth.text),
            }
        )

    return {
        "ä»£è¡¨è€…": representative,
        "å¤§æ ªä¸»": major_shareholders,
        "å½¹å“¡ä¸€è¦§": officers,
    }


def find_elements_by_partial_tag(xbrl_bytes, keyword):
    root = ET.fromstring(xbrl_bytes)
    results = []
    for el in root.iter():
        if keyword.lower() in el.tag.lower():  # å¤§æ–‡å­—å°æ–‡å­—ã‚’ç„¡è¦–ã—ã¦æ¤œç´¢
            text = el.text.strip() if el.text else ""
            results.append((el.tag, text[:100]))  # ãƒ†ã‚­ã‚¹ãƒˆã¯æœ€åˆã®100æ–‡å­—ã ã‘è¡¨ç¤º
    return results


# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆä¾‹ï¼šKDDIï¼šS100QY2Z â€»æå‡ºæ›¸é¡IDï¼‰
doc_id = "S100VJ7H"
xbrl_data = download_edinet_xbrl(doc_id)
# print(xbrl_data.decode("utf-8")[:3000])
# info = parse_xbrl(xbrl_data)

# print("â–  ä»£è¡¨è€…ï¼š", info["ä»£è¡¨è€…"])
# print("â–  å½¹å“¡ä¸€è¦§ï¼š", info["å½¹å“¡ä¸€è¦§"][:200], "â€¦")  # é•·ã„ã®ã§ä¸€éƒ¨ã ã‘
# print("â–  å¤§æ ªä¸»ï¼š", info["å¤§æ ªä¸»"][:200], "â€¦")

# keywords = ["Representative", "Shareholder", "Officer", "Management", "Director"]

# for kw in keywords:
#     print(f"\nğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {kw}")
#     for tag, text in find_elements_by_partial_tag(xbrl_data, kw):
#         print(f"{tag} â†’ {text}")

result = parse_xbrl_latest(xbrl_data)

print("â–  ä»£è¡¨è€…:", result["ä»£è¡¨è€…"])

print("\nâ–  å¤§æ ªä¸»:")
for s in result["å¤§æ ªä¸»"]:
    print(f" - {s['æ°å']} / {s['ä¿æœ‰æ ªæ•°']} / {s['ä¿æœ‰å‰²åˆ']}")

print("\nâ–  å½¹å“¡ä¸€è¦§:")
for o in result["å½¹å“¡ä¸€è¦§"]:
    print(f" - {o['æ°å']}ï¼ˆ{o['è‚©æ›¸ã']}ï¼‰ ç”Ÿå¹´: {o['ç”Ÿå¹´æœˆæ—¥']}")
